{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d2d4f33e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2d4f33e",
        "outputId": "99c49964-3b56-4057-fb85-3341f1cc3007"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5144758830>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Cell 1\n",
        "import random, math\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f10131fa",
      "metadata": {
        "id": "f10131fa"
      },
      "outputs": [],
      "source": [
        "# Cell 2\n",
        "class KmerTokenizer:\n",
        "    def __init__(self, k=4):\n",
        "        self.k = k\n",
        "        bases = ['A','C','G','T','N']\n",
        "        from itertools import product\n",
        "        self.vocab = {'<pad>':0, '<unk>':1}\n",
        "        idx = 2\n",
        "        for kmer in map(''.join, product(bases, repeat=self.k)):\n",
        "            self.vocab[kmer] = idx\n",
        "            idx += 1\n",
        "        self.vocab_size = len(self.vocab)\n",
        "    def encode(self, seq):\n",
        "        seq = seq.upper().replace('U','T')\n",
        "        toks = []\n",
        "        for i in range(0, max(1, len(seq)-self.k+1)):\n",
        "            kmer = seq[i:i+self.k]\n",
        "            toks.append(self.vocab.get(kmer, self.vocab['<unk>']))\n",
        "        return toks[:512]  # Truncate to 512 tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "134a2d36",
      "metadata": {
        "id": "134a2d36"
      },
      "outputs": [],
      "source": [
        "# Cell 3\n",
        "def load_16s_data(csv_path='16S_sequences.csv'):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    data = []\n",
        "    taxa = set()\n",
        "    for _, row in df.iterrows():\n",
        "        # Extract genus from taxonomy string\n",
        "        tax_parts = row['taxonomy'].split()\n",
        "        genus = tax_parts[0] if tax_parts else 'Unknown'\n",
        "        taxa.add(genus)\n",
        "\n",
        "    taxa = sorted(list(taxa))\n",
        "    taxa_to_idx = {t:i for i,t in enumerate(taxa)}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        tax_parts = row['taxonomy'].split()\n",
        "        genus = tax_parts[0] if tax_parts else 'Unknown'\n",
        "        gc = (row['sequence'].count('G') + row['sequence'].count('C')) / len(row['sequence'])\n",
        "        novel = 1 if gc > 0.7 or gc < 0.3 else 0  # Extreme GC content as novelty indicator\n",
        "        data.append({\n",
        "            'seq': row['sequence'],\n",
        "            'tax_idx': taxa_to_idx[genus],\n",
        "            'role_idx': 0,  # Single role for simplicity\n",
        "            'novel': novel\n",
        "        })\n",
        "    return data, taxa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e2ba22fe",
      "metadata": {
        "id": "e2ba22fe"
      },
      "outputs": [],
      "source": [
        "# Cell 4\n",
        "tokenizer = KmerTokenizer(k=4)\n",
        "\n",
        "class ASVDataset(Dataset):\n",
        "    def __init__(self, records, tokenizer):\n",
        "        self.records = records\n",
        "        self.tokenizer = tokenizer\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        rec = self.records[idx]\n",
        "        toks = self.tokenizer.encode(rec['seq'])\n",
        "        return {\n",
        "            'tokens': torch.tensor(toks, dtype=torch.long),\n",
        "            'tax_idx': torch.tensor(rec['tax_idx'], dtype=torch.long),\n",
        "            'role_idx': torch.tensor(rec['role_idx'], dtype=torch.long),\n",
        "            'novel': torch.tensor(rec['novel'], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    tokens = [b['tokens'] for b in batch]\n",
        "    lengths = [t.size(0) for t in tokens]\n",
        "    maxlen = max(lengths)\n",
        "    padded = torch.zeros(len(tokens), maxlen, dtype=torch.long)\n",
        "    mask = torch.zeros(len(tokens), maxlen, dtype=torch.bool)\n",
        "    for i,t in enumerate(tokens):\n",
        "        padded[i,:t.size(0)] = t\n",
        "        mask[i,:t.size(0)] = 1\n",
        "    tax_idx = torch.stack([b['tax_idx'] for b in batch])\n",
        "    role_idx = torch.stack([b['role_idx'] for b in batch])\n",
        "    novel = torch.stack([b['novel'] for b in batch])\n",
        "    # src_key_padding_mask: True where padding\n",
        "    src_key_padding_mask = ~mask\n",
        "    return padded, src_key_padding_mask, tax_idx, role_idx, novel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "982b9a63",
      "metadata": {
        "id": "982b9a63"
      },
      "outputs": [],
      "source": [
        "# Cell 5\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class MultiTaskTaxonomyModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=2, num_layers=1, tax_classes=3, role_classes=3):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.pos = PositionalEncoding(d_model, max_len=512)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=128, dropout=0.1, activation='relu')\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.tax_head = nn.Linear(d_model, tax_classes)\n",
        "        self.role_head = nn.Linear(d_model, role_classes)\n",
        "        self.novel_head = nn.Linear(d_model, 1)\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        emb = self.embed(x) * math.sqrt(self.embed.embedding_dim)\n",
        "        emb = self.pos(emb)\n",
        "        emb_t = emb.transpose(0,1)  # [L,B,D]\n",
        "        enc = self.encoder(emb_t, src_key_padding_mask=src_key_padding_mask)\n",
        "        enc = enc.transpose(0,1)   # [B,L,D]\n",
        "        if src_key_padding_mask is not None:\n",
        "            mask = ~src_key_padding_mask  # True at valid positions\n",
        "            mask = mask.unsqueeze(-1).float()\n",
        "            enc = enc * mask\n",
        "            pooled = enc.sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)\n",
        "        else:\n",
        "            pooled = enc.mean(dim=1)\n",
        "        return {\n",
        "            'tax_logits': self.tax_head(pooled),\n",
        "            'role_logits': self.role_head(pooled),\n",
        "            'novel_logits': self.novel_head(pooled).squeeze(-1)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b4985cee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4985cee",
        "outputId": "f493f553-ca78-48f4-ac9c-0768ac06e3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 27313 sequences with 4307 taxa\n"
          ]
        }
      ],
      "source": [
        "# Cell 6\n",
        "records, taxa = load_16s_data('16S_sequences.csv')\n",
        "print(f'Loaded {len(records)} sequences with {len(taxa)} taxa')\n",
        "split_idx = int(0.8 * len(records))\n",
        "train = records[:split_idx]\n",
        "val = records[split_idx:]\n",
        "\n",
        "train_ds = ASVDataset(train, tokenizer)\n",
        "val_ds = ASVDataset(val, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "9251d9f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9251d9f2",
        "outputId": "c232e28f-8a94-4d7c-b512-c7d97a641c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Cell 7\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MultiTaskTaxonomyModel(vocab_size=tokenizer.vocab_size, d_model=64, nhead=2, num_layers=1, tax_classes=len(taxa)).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "ce = nn.CrossEntropyLoss()\n",
        "bce = nn.BCEWithLogitsLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "f7b1e4b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7b1e4b9",
        "outputId": "8a3028ed-3da7-4c23-f039-9f1da78ce445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train loss 7.4639 | Val loss 7.6403 | Val tax acc 0.027 | Val role acc 1.000\n",
            "Epoch 2 | Train loss 6.8033 | Val loss 7.5541 | Val tax acc 0.050 | Val role acc 1.000\n",
            "Epoch 3 | Train loss 6.5395 | Val loss 7.4046 | Val tax acc 0.070 | Val role acc 1.000\n",
            "Epoch 4 | Train loss 6.2054 | Val loss 7.1359 | Val tax acc 0.076 | Val role acc 1.000\n",
            "Epoch 5 | Train loss 5.8735 | Val loss 6.9244 | Val tax acc 0.092 | Val role acc 1.000\n",
            "Epoch 6 | Train loss 5.5808 | Val loss 6.7407 | Val tax acc 0.105 | Val role acc 1.000\n",
            "Epoch 7 | Train loss 5.3100 | Val loss 6.5141 | Val tax acc 0.121 | Val role acc 1.000\n",
            "Epoch 8 | Train loss 5.0574 | Val loss 6.3385 | Val tax acc 0.133 | Val role acc 1.000\n",
            "Epoch 9 | Train loss 4.8228 | Val loss 6.1672 | Val tax acc 0.151 | Val role acc 1.000\n",
            "Epoch 10 | Train loss 4.6038 | Val loss 5.9991 | Val tax acc 0.163 | Val role acc 1.000\n",
            "Epoch 11 | Train loss 4.3980 | Val loss 5.8592 | Val tax acc 0.178 | Val role acc 1.000\n",
            "Epoch 12 | Train loss 4.1944 | Val loss 5.7126 | Val tax acc 0.208 | Val role acc 1.000\n",
            "Epoch 13 | Train loss 4.0035 | Val loss 5.5897 | Val tax acc 0.215 | Val role acc 1.000\n",
            "Epoch 14 | Train loss 3.8218 | Val loss 5.4149 | Val tax acc 0.240 | Val role acc 1.000\n",
            "Epoch 15 | Train loss 3.6453 | Val loss 5.4522 | Val tax acc 0.256 | Val role acc 1.000\n",
            "Epoch 16 | Train loss 3.4808 | Val loss 5.2827 | Val tax acc 0.267 | Val role acc 1.000\n",
            "Epoch 17 | Train loss 3.3259 | Val loss 5.1117 | Val tax acc 0.280 | Val role acc 1.000\n",
            "Epoch 18 | Train loss 3.1716 | Val loss 5.2212 | Val tax acc 0.290 | Val role acc 1.000\n",
            "Epoch 19 | Train loss 3.0350 | Val loss 5.0853 | Val tax acc 0.297 | Val role acc 1.000\n",
            "Epoch 20 | Train loss 2.9024 | Val loss 5.0092 | Val tax acc 0.312 | Val role acc 1.000\n"
          ]
        }
      ],
      "source": [
        "# Cell 8\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct_tax = 0\n",
        "    correct_role = 0\n",
        "    with torch.no_grad():\n",
        "        for padded, src_key_padding_mask, tax_idx, role_idx, novel in loader:\n",
        "            padded = padded.to(device); src_key_padding_mask = src_key_padding_mask.to(device)\n",
        "            tax_idx = tax_idx.to(device); role_idx = role_idx.to(device)\n",
        "            out = model(padded, src_key_padding_mask=src_key_padding_mask)\n",
        "            correct_tax += (out['tax_logits'].argmax(dim=1) == tax_idx).sum().item()\n",
        "            correct_role += (out['role_logits'].argmax(dim=1) == role_idx).sum().item()\n",
        "            total += tax_idx.size(0)\n",
        "    return correct_tax/total, correct_role/total\n",
        "\n",
        "max_epochs = 20\n",
        "early_stopping_patience = 3\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for padded, src_key_padding_mask, tax_idx, role_idx, novel in train_loader:\n",
        "        padded = padded.to(device); src_key_padding_mask = src_key_padding_mask.to(device)\n",
        "        tax_idx = tax_idx.to(device); role_idx = role_idx.to(device); novel = novel.to(device)\n",
        "        out = model(padded, src_key_padding_mask=src_key_padding_mask)\n",
        "        loss = ce(out['tax_logits'], tax_idx) + ce(out['role_logits'], role_idx) + 1.5*bce(out['novel_logits'], novel)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        running += loss.item() * tax_idx.size(0)\n",
        "    avg_train_loss = running / len(train)\n",
        "\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for padded, src_key_padding_mask, tax_idx, role_idx, novel in val_loader:\n",
        "            padded = padded.to(device); src_key_padding_mask = src_key_padding_mask.to(device)\n",
        "            tax_idx = tax_idx.to(device); role_idx = role_idx.to(device); novel = novel.to(device)\n",
        "            out = model(padded, src_key_padding_mask=src_key_padding_mask)\n",
        "            loss = ce(out['tax_logits'], tax_idx) + ce(out['role_logits'], role_idx) + 1.5*bce(out['novel_logits'], novel)\n",
        "            running_val_loss += loss.item() * tax_idx.size(0)\n",
        "    avg_val_loss = running_val_loss / len(val)\n",
        "\n",
        "    tax_acc, role_acc = evaluate(val_loader)\n",
        "    print(f\"Epoch {epoch} | Train loss {avg_train_loss:.4f} | Val loss {avg_val_loss:.4f} | Val tax acc {tax_acc:.3f} | Val role acc {role_acc:.3f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    # if avg_val_loss < best_val_loss:\n",
        "    #     best_val_loss = avg_val_loss\n",
        "    #     epochs_no_improve = 0\n",
        "    # else:\n",
        "    #     epochs_no_improve += 1\n",
        "\n",
        "    # if epochs_no_improve == early_stopping_patience:\n",
        "    #     print(f\"Early stopping after {epoch} epochs.\")\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "a2e044ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e044ab",
        "outputId": "df879226-7d98-4492-94af-6dbec7ab51d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True tax: Alloscardovia | Pred tax: Eubacterium | Novel score: 0.397\n",
            "True tax: Bifidobacterium | Pred tax: Eubacterium | Novel score: 0.397\n",
            "True tax: Galliscardovia | Pred tax: Eubacterium | Novel score: 0.397\n",
            "True tax: Roseomonas | Pred tax: Eubacterium | Novel score: 0.397\n",
            "True tax: Chthonobacter | Pred tax: Eubacterium | Novel score: 0.397\n",
            "True tax: Actinophytocola | Pred tax: Eubacterium | Novel score: 0.397\n"
          ]
        }
      ],
      "source": [
        "# Cell 9\n",
        "model.eval()\n",
        "for ex in val[:6]:\n",
        "    toks = torch.tensor(tokenizer.encode(ex['seq']), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    mask = torch.zeros(1, toks.size(1), dtype=torch.bool).to(device)\n",
        "    out = model(toks, src_key_padding_mask=~mask)\n",
        "    tax_pred = taxa[out['tax_logits'].argmax(dim=1).item()]\n",
        "    novel_score = torch.sigmoid(out['novel_logits']).item()\n",
        "    print(f\"True tax: {taxa[ex['tax_idx']]} | Pred tax: {tax_pred} | Novel score: {novel_score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c2ccacff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ccacff",
        "outputId": "a380f8f3-c5c5-44f8-e499-9191078a6d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to multitask_model_demo_small.pth\n"
          ]
        }
      ],
      "source": [
        "# Cell 10\n",
        "torch.save({'model_state_dict': model.state_dict(), 'tokenizer_vocab': tokenizer.vocab}, 'multitask_model_demo_small.pth')\n",
        "print(\"Saved model to multitask_model_demo_small.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "fix_cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fix_cell",
        "outputId": "e922b977-40b0-453a-a303-d20b308e8c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 27313 sequences with 4307 taxa\n",
            "First sequence tokenized to 512 tokens\n",
            "Everything recreated successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Fix Cell - Recreate everything\n",
        "del tokenizer, train_ds, val_ds, train_loader, val_loader, model\n",
        "\n",
        "tokenizer = KmerTokenizer(k=4)\n",
        "records, taxa = load_16s_data('16S_sequences.csv')\n",
        "print(f'Loaded {len(records)} sequences with {len(taxa)} taxa')\n",
        "\n",
        "# Test tokenization\n",
        "test_tokens = tokenizer.encode(records[0]['seq'])\n",
        "print(f'First sequence tokenized to {len(test_tokens)} tokens')\n",
        "\n",
        "split_idx = int(0.8 * len(records))\n",
        "train = records[:split_idx]\n",
        "val = records[split_idx:]\n",
        "\n",
        "train_ds = ASVDataset(train, tokenizer)\n",
        "val_ds = ASVDataset(val, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MultiTaskTaxonomyModel(vocab_size=tokenizer.vocab_size, d_model=64, nhead=2, num_layers=1, tax_classes=len(taxa)).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "ce = nn.CrossEntropyLoss()\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "print('Everything recreated successfully')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "prediction_cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prediction_cell",
        "outputId": "71457506-027c-479a-fa09-c87eaf2247d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw tax logits: tensor([[-4.2747, -4.3353,  0.8509,  ..., -1.0977,  1.0979, -1.4322]],\n",
            "       device='cuda:0')\n",
            "Sequence length: 1467 bp\n",
            "Tokenized length: 512 k-mers\n",
            "Novel score: 0.003\n",
            "\n",
            "Top 3 Predictions:\n",
            "1. Chitinophaga (9.4%)\n",
            "2. Pedobacter (6.8%)\n",
            "3. Halolamina (3.6%)\n"
          ]
        }
      ],
      "source": [
        "# Prediction Cell\n",
        "def predict_taxonomy(sequence, model, tokenizer, taxa):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(sequence)\n",
        "    input_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        print(f\"Raw tax logits: {output['tax_logits']}\") # Added print statement\n",
        "        tax_probs = torch.softmax(output['tax_logits'], dim=1)\n",
        "        novel_score = torch.sigmoid(output['novel_logits']).item()\n",
        "\n",
        "        top_probs, top_indices = torch.topk(tax_probs, k=min(3, len(taxa)))\n",
        "\n",
        "        print(f'Sequence length: {len(sequence)} bp')\n",
        "        print(f'Tokenized length: {len(tokens)} k-mers')\n",
        "        print(f'Novel score: {novel_score:.3f}')\n",
        "        print('\\nTop 3 Predictions:')\n",
        "        for i, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0])):\n",
        "            genus = taxa[idx.item()]\n",
        "            confidence = prob.item() * 100\n",
        "            print(f'{i+1}. {genus} ({confidence:.1f}%)')\n",
        "\n",
        "# Example prediction\n",
        "test_seq = 'GATTATGGCTCAGAACGAACGCTGGCGGCAGGCCTAACACATGCAAGTCGAGCGCGCCTTTCGGGGCGAGCGGCGGACGGGTTAGTAACGCGTGGGAATGTACCCTTTTCTACGGAATAGCCTCGGGAAACTGAGATTAATACCGTATACGCCCCCCCAATCAAATTTCATTTGATTGAATTTTCAGTCATATCAAATTCCGAATGGAATTTGATGGGGGGGGAAAGATTTATCGGAGAAGGATCAGCCCGCGTTAGATTAGATAGTTGGTGGGGTAATGGCCTACCAAGTCTACGATCTATAGCTGGTTTGAGAGGATGATCAGCAACACTGGGACTGAGACACGGCCCAGACTCCTACGGGAGGCAGCAGTGGGGAATCTTAGACAATGGGCGCAAGCCTGATCTAGCCATGCCGCGTGAGTGAAGAAGGCCTTAGGGTCGTAAAGCTCTTTCAGTGGGGAAGATAATGACGGTACCCACAGAAGAAACCCCGGCTAACTCCGTGCCAGCAGCCGCGGTAATACGGAGGGGGTTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGTACGTAGGCGGATTAGCAAGTTAGAGGTGAAATCCCAGGGCTCAACCTTGGAACTGCCTTTAAAACTGCTAGTCTTGAGTTCGAGAGAGGTGAGTGGAATTCCGAGTGTAGAGGTGAAATTCGTAGATATTCGGAGGAACACCAGTGGCGAAGGCGGCTCACTGGCTCGATACTGACGCTGAGGTACGAAAGCGTGGGGAGCAAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGATGAGAGCTAGTCGTCGGGTTGCATGCAATTCGGTGACGCAGTTAACGCATTAAGCTCTCCGCCTGGGGAGTACGGTCGCAAGATTAAAACTCAAAGGAATTGACGGGGGCCCGCACAAGCGGTGGAGCATGTGGTTTAATTCGAAGCAACGCGCAGAACCTTACCAACCCTTGACATACCTGTCGCGGCCCGAGAGATCGGGCTTTCAGTTCGGCTGGACAGGATACAGGTGCTGCATGGCTGTCGTCAGCTCGTGTCGTGAGATGTTCGGTTAAGTCCGGCAACGAGCGCAACCCCTGCCTTTAGTTGCCAGCATTCAGTTGGGCACTCTAGAGGGACCGCCGGTGATAAGCCGGAGGAAGGTGGGGATGACGTCAAGTCCTCATGGCCCTTACGGGTTGGGCTACACACGTGCTACAATGGTAGTGACAATGGGTTAATCCCAAAAAGCTATCTCAGTTCGGATTGTCCTCTGCAACTCGAGGGCATGAAGTTGGAATCGCTAGTAATCGCGTAACAGCATGACGCGGTGAATACGTTCCCGGGCCTTGTACACACCGCCCGTCACACCATGGGAATTGGATCTACCCGAAGGCCGTGCGCTAATTTGGCAGCGGACCACGGTAGGTTCAGTGACTGGGGTGAAGTCGTAACAAGG'\n",
        "predict_taxonomy(test_seq, model, tokenizer, taxa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef5e5b3a",
        "outputId": "ff402211-e1d2-464e-bff2-f4ec4c8c8cbe"
      },
      "source": [
        "# Load the saved model state dictionary\n",
        "import torch\n",
        "import math\n",
        "from itertools import product\n",
        "from torch import nn\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Define KmerTokenizer class (assuming it's not globally available)\n",
        "class KmerTokenizer:\n",
        "    def __init__(self, k=4):\n",
        "        self.k = k\n",
        "        bases = ['A','C','G','T','N']\n",
        "        from itertools import product\n",
        "        self.vocab = {'<pad>':0, '<unk>':1}\n",
        "        idx = 2\n",
        "        for kmer in map(''.join, product(bases, repeat=self.k)):\n",
        "            self.vocab[kmer] = idx\n",
        "            idx += 1\n",
        "        self.vocab_size = len(self.vocab)\n",
        "    def encode(self, seq):\n",
        "        seq = seq.upper().replace('U','T')\n",
        "        toks = []\n",
        "        for i in range(0, max(1, len(seq)-self.k+1)):\n",
        "            kmer = seq[i:i+self.k]\n",
        "            toks.append(self.vocab.get(kmer, self.vocab['<unk>']))\n",
        "        return toks[:512]  # Truncate to 512 tokens\n",
        "\n",
        "# Define PositionalEncoding class (assuming it's not globally available)\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# Define MultiTaskTaxonomyModel class (assuming it's not globally available)\n",
        "class MultiTaskTaxonomyModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=2, num_layers=1, tax_classes=3, role_classes=3):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.pos = PositionalEncoding(d_model, max_len=512)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=128, dropout=0.1, activation='relu')\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.tax_head = nn.Linear(d_model, tax_classes)\n",
        "        self.role_head = nn.Linear(d_model, role_classes)\n",
        "        self.novel_head = nn.Linear(d_model, 1)\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        emb = self.embed(x) * math.sqrt(self.embed.embedding_dim)\n",
        "        emb = self.pos(emb)\n",
        "        emb_t = emb.transpose(0,1)  # [L,B,D]\n",
        "        enc = self.encoder(emb_t, src_key_padding_mask=src_key_padding_mask)\n",
        "        enc = enc.transpose(0,1)   # [B,L,D]\n",
        "        if src_key_padding_mask is not None:\n",
        "            mask = ~src_key_padding_mask  # True at valid positions\n",
        "            mask = mask.unsqueeze(-1).float()\n",
        "            enc = enc * mask\n",
        "            pooled = enc.sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)\n",
        "        else:\n",
        "            pooled = enc.mean(dim=1)\n",
        "        return {\n",
        "            'tax_logits': self.tax_head(pooled),\n",
        "            'role_logits': self.role_head(pooled),\n",
        "            'novel_logits': self.novel_head(pooled).squeeze(-1)\n",
        "        }\n",
        "\n",
        "# Assuming 'taxa' is a list containing the unique taxa from your data\n",
        "# and its length represents the correct number of tax_classes (4307)\n",
        "# If 'taxa' is not defined in a previous cell, you might need to load your data again to get it.\n",
        "# For now, I will assume 'taxa' is available.\n",
        "# If you still get a NameError for 'taxa', you might need to add a cell to load the data.\n",
        "\n",
        "# Load data and get taxa to determine the correct number of tax_classes\n",
        "def load_16s_data(csv_path='16S_sequences.csv'):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    data = []\n",
        "    taxa = set()\n",
        "    for _, row in df.iterrows():\n",
        "        # Extract genus from taxonomy string\n",
        "        tax_parts = row['taxonomy'].split()\n",
        "        genus = tax_parts[0] if tax_parts else 'Unknown'\n",
        "        taxa.add(genus)\n",
        "\n",
        "    taxa = sorted(list(taxa))\n",
        "    taxa_to_idx = {t:i for i,t in enumerate(taxa)}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        tax_parts = row['taxonomy'].split()\n",
        "        genus = tax_parts[0] if tax_parts else 'Unknown'\n",
        "        gc = (row['sequence'].count('G') + row['sequence'].count('C')) / len(row['sequence'])\n",
        "        novel = 1 if gc > 0.7 or gc < 0.3 else 0  # Extreme GC content as novelty indicator\n",
        "        data.append({\n",
        "            'seq': row['sequence'],\n",
        "            'tax_idx': taxa_to_idx[genus],\n",
        "            'role_idx': 0,  # Single role for simplicity\n",
        "            'novel': novel\n",
        "        })\n",
        "    return data, taxa\n",
        "\n",
        "records, taxa = load_16s_data('16S_sequences.csv') # Load data and define taxa\n",
        "\n",
        "# Re-initialize the model with the correct number of tax_classes\n",
        "tokenizer = KmerTokenizer(k=4) # Define tokenizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Define device\n",
        "model = MultiTaskTaxonomyModel(vocab_size=tokenizer.vocab_size, d_model=64, nhead=2, num_layers=1, tax_classes=len(taxa)).to(device)\n",
        "checkpoint = torch.load('multitask_model_demo_small.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(\"Model state dictionary loaded successfully.\")"
      ],
      "id": "ef5e5b3a",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state dictionary loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "pPdIylVSiotO"
      },
      "id": "pPdIylVSiotO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "59cfe303",
        "outputId": "6ca95230-ec12-4086-8925-6830db4f470e"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('multitask_model_demo_small.pth')"
      ],
      "id": "59cfe303",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b2b4c5b8-201c-4eb1-9907-0e2bf9a228db\", \"multitask_model_demo_small.pth\", 1564885)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}